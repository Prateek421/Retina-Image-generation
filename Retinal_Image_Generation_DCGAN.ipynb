{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1056729,"sourceType":"datasetVersion","datasetId":585327},{"sourceId":1063445,"sourceType":"datasetVersion","datasetId":589983},{"sourceId":3726698,"sourceType":"datasetVersion","datasetId":2228455}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicki/ng run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:35.941558Z","iopub.execute_input":"2025-05-05T17:46:35.942006Z","iopub.status.idle":"2025-05-05T17:46:37.093899Z","shell.execute_reply.started":"2025-05-05T17:46:35.941963Z","shell.execute_reply":"2025-05-05T17:46:37.093230Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"/kaggle/input/stare-dataset/im0296.ppm\n/kaggle/input/stare-dataset/im0372.ppm\n/kaggle/input/stare-dataset/im0301.ppm\n/kaggle/input/stare-dataset/im0209.ppm\n/kaggle/input/stare-dataset/im0106.ppm\n/kaggle/input/stare-dataset/im0089.ppm\n/kaggle/input/stare-dataset/im0232.ppm\n/kaggle/input/stare-dataset/im0069.ppm\n/kaggle/input/stare-dataset/im0342.ppm\n/kaggle/input/stare-dataset/im0395.ppm\n/kaggle/input/stare-dataset/im0302.ppm\n/kaggle/input/stare-dataset/im0087.ppm\n/kaggle/input/stare-dataset/im0227.ppm\n/kaggle/input/stare-dataset/im0350.ppm\n/kaggle/input/stare-dataset/im0074.ppm\n/kaggle/input/stare-dataset/im0250.ppm\n/kaggle/input/stare-dataset/im0333.ppm\n/kaggle/input/stare-dataset/im0288.ppm\n/kaggle/input/stare-dataset/im0192.ppm\n/kaggle/input/stare-dataset/im0399.ppm\n/kaggle/input/stare-dataset/im0247.ppm\n/kaggle/input/stare-dataset/im0007.ppm\n/kaggle/input/stare-dataset/im0240.ppm\n/kaggle/input/stare-dataset/im0071.ppm\n/kaggle/input/stare-dataset/im0343.ppm\n/kaggle/input/stare-dataset/im0379.ppm\n/kaggle/input/stare-dataset/im0401.ppm\n/kaggle/input/stare-dataset/im0385.ppm\n/kaggle/input/stare-dataset/im0070.ppm\n/kaggle/input/stare-dataset/im0161.ppm\n/kaggle/input/stare-dataset/im0220.ppm\n/kaggle/input/stare-dataset/im0381.ppm\n/kaggle/input/stare-dataset/im0038.ppm\n/kaggle/input/stare-dataset/im0037.ppm\n/kaggle/input/stare-dataset/im0313.ppm\n/kaggle/input/stare-dataset/im0076.ppm\n/kaggle/input/stare-dataset/im0103.ppm\n/kaggle/input/stare-dataset/im0317.ppm\n/kaggle/input/stare-dataset/im0238.ppm\n/kaggle/input/stare-dataset/im0363.ppm\n/kaggle/input/stare-dataset/im0341.ppm\n/kaggle/input/stare-dataset/im0234.ppm\n/kaggle/input/stare-dataset/im0390.ppm\n/kaggle/input/stare-dataset/im0030.ppm\n/kaggle/input/stare-dataset/im0006.ppm\n/kaggle/input/stare-dataset/im0095.ppm\n/kaggle/input/stare-dataset/im0133.ppm\n/kaggle/input/stare-dataset/im0346.ppm\n/kaggle/input/stare-dataset/im0294.ppm\n/kaggle/input/stare-dataset/im0032.ppm\n/kaggle/input/stare-dataset/im0258.ppm\n/kaggle/input/stare-dataset/im0033.ppm\n/kaggle/input/stare-dataset/im0141.ppm\n/kaggle/input/stare-dataset/im0175.ppm\n/kaggle/input/stare-dataset/im0182.ppm\n/kaggle/input/stare-dataset/im0104.ppm\n/kaggle/input/stare-dataset/im0171.ppm\n/kaggle/input/stare-dataset/im0223.ppm\n/kaggle/input/stare-dataset/im0078.ppm\n/kaggle/input/stare-dataset/im0241.ppm\n/kaggle/input/stare-dataset/im0299.ppm\n/kaggle/input/stare-dataset/im0075.ppm\n/kaggle/input/stare-dataset/im0016.ppm\n/kaggle/input/stare-dataset/im0304.ppm\n/kaggle/input/stare-dataset/im0183.ppm\n/kaggle/input/stare-dataset/im0275.ppm\n/kaggle/input/stare-dataset/im0216.ppm\n/kaggle/input/stare-dataset/im0274.ppm\n/kaggle/input/stare-dataset/im0326.ppm\n/kaggle/input/stare-dataset/im0389.ppm\n/kaggle/input/stare-dataset/im0297.ppm\n/kaggle/input/stare-dataset/im0149.ppm\n/kaggle/input/stare-dataset/im0337.ppm\n/kaggle/input/stare-dataset/im0036.ppm\n/kaggle/input/stare-dataset/im0284.ppm\n/kaggle/input/stare-dataset/im0117.ppm\n/kaggle/input/stare-dataset/im0082.ppm\n/kaggle/input/stare-dataset/im0044.ppm\n/kaggle/input/stare-dataset/im0190.ppm\n/kaggle/input/stare-dataset/im0225.ppm\n/kaggle/input/stare-dataset/im0132.ppm\n/kaggle/input/stare-dataset/im0307.ppm\n/kaggle/input/stare-dataset/im0126.ppm\n/kaggle/input/stare-dataset/im0127.ppm\n/kaggle/input/stare-dataset/im0305.ppm\n/kaggle/input/stare-dataset/im0362.ppm\n/kaggle/input/stare-dataset/im0358.ppm\n/kaggle/input/stare-dataset/im0394.ppm\n/kaggle/input/stare-dataset/im0170.ppm\n/kaggle/input/stare-dataset/im0112.ppm\n/kaggle/input/stare-dataset/im0064.ppm\n/kaggle/input/stare-dataset/im0160.ppm\n/kaggle/input/stare-dataset/im0336.ppm\n/kaggle/input/stare-dataset/im0377.ppm\n/kaggle/input/stare-dataset/im0143.ppm\n/kaggle/input/stare-dataset/im0219.ppm\n/kaggle/input/stare-dataset/im0230.ppm\n/kaggle/input/stare-dataset/im0009.ppm\n/kaggle/input/stare-dataset/im0215.ppm\n/kaggle/input/stare-dataset/im0244.ppm\n/kaggle/input/stare-dataset/im0094.ppm\n/kaggle/input/stare-dataset/im0204.ppm\n/kaggle/input/stare-dataset/im0015.ppm\n/kaggle/input/stare-dataset/im0206.ppm\n/kaggle/input/stare-dataset/im0266.ppm\n/kaggle/input/stare-dataset/im0163.ppm\n/kaggle/input/stare-dataset/im0328.ppm\n/kaggle/input/stare-dataset/im0169.ppm\n/kaggle/input/stare-dataset/im0252.ppm\n/kaggle/input/stare-dataset/im0293.ppm\n/kaggle/input/stare-dataset/im0392.ppm\n/kaggle/input/stare-dataset/im0318.ppm\n/kaggle/input/stare-dataset/im0285.ppm\n/kaggle/input/stare-dataset/im0111.ppm\n/kaggle/input/stare-dataset/im0119.ppm\n/kaggle/input/stare-dataset/im0356.ppm\n/kaggle/input/stare-dataset/im0187.ppm\n/kaggle/input/stare-dataset/im0242.ppm\n/kaggle/input/stare-dataset/im0271.ppm\n/kaggle/input/stare-dataset/im0245.ppm\n/kaggle/input/stare-dataset/im0391.ppm\n/kaggle/input/stare-dataset/im0131.ppm\n/kaggle/input/stare-dataset/im0228.ppm\n/kaggle/input/stare-dataset/im0029.ppm\n/kaggle/input/stare-dataset/im0178.ppm\n/kaggle/input/stare-dataset/im0387.ppm\n/kaggle/input/stare-dataset/im0371.ppm\n/kaggle/input/stare-dataset/im0386.ppm\n/kaggle/input/stare-dataset/im0048.ppm\n/kaggle/input/stare-dataset/im0050.ppm\n/kaggle/input/stare-dataset/im0005.ppm\n/kaggle/input/stare-dataset/im0262.ppm\n/kaggle/input/stare-dataset/im0013.ppm\n/kaggle/input/stare-dataset/im0259.ppm\n/kaggle/input/stare-dataset/im0273.ppm\n/kaggle/input/stare-dataset/im0348.ppm\n/kaggle/input/stare-dataset/im0177.ppm\n/kaggle/input/stare-dataset/im0085.ppm\n/kaggle/input/stare-dataset/im0002.ppm\n/kaggle/input/stare-dataset/im0123.ppm\n/kaggle/input/stare-dataset/im0210.ppm\n/kaggle/input/stare-dataset/im0142.ppm\n/kaggle/input/stare-dataset/im0181.ppm\n/kaggle/input/stare-dataset/im0110.ppm\n/kaggle/input/stare-dataset/im0364.ppm\n/kaggle/input/stare-dataset/im0043.ppm\n/kaggle/input/stare-dataset/im0150.ppm\n/kaggle/input/stare-dataset/im0186.ppm\n/kaggle/input/stare-dataset/im0383.ppm\n/kaggle/input/stare-dataset/im0164.ppm\n/kaggle/input/stare-dataset/im0207.ppm\n/kaggle/input/stare-dataset/im0257.ppm\n/kaggle/input/stare-dataset/im0279.ppm\n/kaggle/input/stare-dataset/im0096.ppm\n/kaggle/input/stare-dataset/im0292.ppm\n/kaggle/input/stare-dataset/im0165.ppm\n/kaggle/input/stare-dataset/im0153.ppm\n/kaggle/input/stare-dataset/im0026.ppm\n/kaggle/input/stare-dataset/im0051.ppm\n/kaggle/input/stare-dataset/im0034.ppm\n/kaggle/input/stare-dataset/im0281.ppm\n/kaggle/input/stare-dataset/im0331.ppm\n/kaggle/input/stare-dataset/im0168.ppm\n/kaggle/input/stare-dataset/im0208.ppm\n/kaggle/input/stare-dataset/im0349.ppm\n/kaggle/input/stare-dataset/im0068.ppm\n/kaggle/input/stare-dataset/im0101.ppm\n/kaggle/input/stare-dataset/im0229.ppm\n/kaggle/input/stare-dataset/im0214.ppm\n/kaggle/input/stare-dataset/im0193.ppm\n/kaggle/input/stare-dataset/im0368.ppm\n/kaggle/input/stare-dataset/im0031.ppm\n/kaggle/input/stare-dataset/im0269.ppm\n/kaggle/input/stare-dataset/im0129.ppm\n/kaggle/input/stare-dataset/im0360.ppm\n/kaggle/input/stare-dataset/im0018.ppm\n/kaggle/input/stare-dataset/im0373.ppm\n/kaggle/input/stare-dataset/im0378.ppm\n/kaggle/input/stare-dataset/im0107.ppm\n/kaggle/input/stare-dataset/im0352.ppm\n/kaggle/input/stare-dataset/im0283.ppm\n/kaggle/input/stare-dataset/im0365.ppm\n/kaggle/input/stare-dataset/im0060.ppm\n/kaggle/input/stare-dataset/im0375.ppm\n/kaggle/input/stare-dataset/im0124.ppm\n/kaggle/input/stare-dataset/im0315.ppm\n/kaggle/input/stare-dataset/im0118.ppm\n/kaggle/input/stare-dataset/im0191.ppm\n/kaggle/input/stare-dataset/im0008.ppm\n/kaggle/input/stare-dataset/im0351.ppm\n/kaggle/input/stare-dataset/im0093.ppm\n/kaggle/input/stare-dataset/im0158.ppm\n/kaggle/input/stare-dataset/im0211.ppm\n/kaggle/input/stare-dataset/im0217.ppm\n/kaggle/input/stare-dataset/im0162.ppm\n/kaggle/input/stare-dataset/im0056.ppm\n/kaggle/input/stare-dataset/im0135.ppm\n/kaggle/input/stare-dataset/im0159.ppm\n/kaggle/input/stare-dataset/im0079.ppm\n/kaggle/input/stare-dataset/im0256.ppm\n/kaggle/input/stare-dataset/im0157.ppm\n/kaggle/input/stare-dataset/im0080.ppm\n/kaggle/input/stare-dataset/im0309.ppm\n/kaggle/input/stare-dataset/im0202.ppm\n/kaggle/input/stare-dataset/im0286.ppm\n/kaggle/input/stare-dataset/im0004.ppm\n/kaggle/input/stare-dataset/im0236.ppm\n/kaggle/input/stare-dataset/im0022.ppm\n/kaggle/input/stare-dataset/im0017.ppm\n/kaggle/input/stare-dataset/im0277.ppm\n/kaggle/input/stare-dataset/im0145.ppm\n/kaggle/input/stare-dataset/im0295.ppm\n/kaggle/input/stare-dataset/im0251.ppm\n/kaggle/input/stare-dataset/im0246.ppm\n/kaggle/input/stare-dataset/im0357.ppm\n/kaggle/input/stare-dataset/im0231.ppm\n/kaggle/input/stare-dataset/im0138.ppm\n/kaggle/input/stare-dataset/im0045.ppm\n/kaggle/input/stare-dataset/im0176.ppm\n/kaggle/input/stare-dataset/im0261.ppm\n/kaggle/input/stare-dataset/im0344.ppm\n/kaggle/input/stare-dataset/im0014.ppm\n/kaggle/input/stare-dataset/im0310.ppm\n/kaggle/input/stare-dataset/im0065.ppm\n/kaggle/input/stare-dataset/im0010.ppm\n/kaggle/input/stare-dataset/im0400.ppm\n/kaggle/input/stare-dataset/im0057.ppm\n/kaggle/input/stare-dataset/im0148.ppm\n/kaggle/input/stare-dataset/im0105.ppm\n/kaggle/input/stare-dataset/im0278.ppm\n/kaggle/input/stare-dataset/im0049.ppm\n/kaggle/input/stare-dataset/im0369.ppm\n/kaggle/input/stare-dataset/im0338.ppm\n/kaggle/input/stare-dataset/im0308.ppm\n/kaggle/input/stare-dataset/im0083.ppm\n/kaggle/input/stare-dataset/im0086.ppm\n/kaggle/input/stare-dataset/im0330.ppm\n/kaggle/input/stare-dataset/im0139.ppm\n/kaggle/input/stare-dataset/im0024.ppm\n/kaggle/input/stare-dataset/im0041.ppm\n/kaggle/input/stare-dataset/im0021.ppm\n/kaggle/input/stare-dataset/im0154.ppm\n/kaggle/input/stare-dataset/im0289.ppm\n/kaggle/input/stare-dataset/im0226.ppm\n/kaggle/input/stare-dataset/im0263.ppm\n/kaggle/input/stare-dataset/im0321.ppm\n/kaggle/input/stare-dataset/im0316.ppm\n/kaggle/input/stare-dataset/im0012.ppm\n/kaggle/input/stare-dataset/im0194.ppm\n/kaggle/input/stare-dataset/im0249.ppm\n/kaggle/input/stare-dataset/im0397.ppm\n/kaggle/input/stare-dataset/im0332.ppm\n/kaggle/input/stare-dataset/im0345.ppm\n/kaggle/input/stare-dataset/im0028.ppm\n/kaggle/input/stare-dataset/im0237.ppm\n/kaggle/input/stare-dataset/im0020.ppm\n/kaggle/input/stare-dataset/im0334.ppm\n/kaggle/input/stare-dataset/im0393.ppm\n/kaggle/input/stare-dataset/im0280.ppm\n/kaggle/input/stare-dataset/im0361.ppm\n/kaggle/input/stare-dataset/im0100.ppm\n/kaggle/input/stare-dataset/im0355.ppm\n/kaggle/input/stare-dataset/im0084.ppm\n/kaggle/input/stare-dataset/im0053.ppm\n/kaggle/input/stare-dataset/im0081.ppm\n/kaggle/input/stare-dataset/im0203.ppm\n/kaggle/input/stare-dataset/im0367.ppm\n/kaggle/input/stare-dataset/im0222.ppm\n/kaggle/input/stare-dataset/im0046.ppm\n/kaggle/input/stare-dataset/im0088.ppm\n/kaggle/input/stare-dataset/im0359.ppm\n/kaggle/input/stare-dataset/im0054.ppm\n/kaggle/input/stare-dataset/im0325.ppm\n/kaggle/input/stare-dataset/im0253.ppm\n/kaggle/input/stare-dataset/im0300.ppm\n/kaggle/input/stare-dataset/im0290.ppm\n/kaggle/input/stare-dataset/im0347.ppm\n/kaggle/input/stare-dataset/im0173.ppm\n/kaggle/input/stare-dataset/im0303.ppm\n/kaggle/input/stare-dataset/im0195.ppm\n/kaggle/input/stare-dataset/im0134.ppm\n/kaggle/input/stare-dataset/im0298.ppm\n/kaggle/input/stare-dataset/im0380.ppm\n/kaggle/input/stare-dataset/im0137.ppm\n/kaggle/input/stare-dataset/im0199.ppm\n/kaggle/input/stare-dataset/im0224.ppm\n/kaggle/input/stare-dataset/im0058.ppm\n/kaggle/input/stare-dataset/im0323.ppm\n/kaggle/input/stare-dataset/im0063.ppm\n/kaggle/input/stare-dataset/im0196.ppm\n/kaggle/input/stare-dataset/im0097.ppm\n/kaggle/input/stare-dataset/im0396.ppm\n/kaggle/input/stare-dataset/im0027.ppm\n/kaggle/input/stare-dataset/im0091.ppm\n/kaggle/input/stare-dataset/im0152.ppm\n/kaggle/input/stare-dataset/im0324.ppm\n/kaggle/input/stare-dataset/im0019.ppm\n/kaggle/input/stare-dataset/im0276.ppm\n/kaggle/input/stare-dataset/im0042.ppm\n/kaggle/input/stare-dataset/im0003.ppm\n/kaggle/input/stare-dataset/im0233.ppm\n/kaggle/input/stare-dataset/im0113.ppm\n/kaggle/input/stare-dataset/im0221.ppm\n/kaggle/input/stare-dataset/im0366.ppm\n/kaggle/input/stare-dataset/im0061.ppm\n/kaggle/input/stare-dataset/im0077.ppm\n/kaggle/input/stare-dataset/im0384.ppm\n/kaggle/input/stare-dataset/im0146.ppm\n/kaggle/input/stare-dataset/im0235.ppm\n/kaggle/input/stare-dataset/im0122.ppm\n/kaggle/input/stare-dataset/im0011.ppm\n/kaggle/input/stare-dataset/im0185.ppm\n/kaggle/input/stare-dataset/im0151.ppm\n/kaggle/input/stare-dataset/im0254.ppm\n/kaggle/input/stare-dataset/im0023.ppm\n/kaggle/input/stare-dataset/im0201.ppm\n/kaggle/input/stare-dataset/im0339.ppm\n/kaggle/input/stare-dataset/im0125.ppm\n/kaggle/input/stare-dataset/im0062.ppm\n/kaggle/input/stare-dataset/im0052.ppm\n/kaggle/input/stare-dataset/im0267.ppm\n/kaggle/input/stare-dataset/im0115.ppm\n/kaggle/input/stare-dataset/im0120.ppm\n/kaggle/input/stare-dataset/im0025.ppm\n/kaggle/input/stare-dataset/im0374.ppm\n/kaggle/input/stare-dataset/im0287.ppm\n/kaggle/input/stare-dataset/im0255.ppm\n/kaggle/input/stare-dataset/im0282.ppm\n/kaggle/input/stare-dataset/im0128.ppm\n/kaggle/input/stare-dataset/im0322.ppm\n/kaggle/input/stare-dataset/im0072.ppm\n/kaggle/input/stare-dataset/im0205.ppm\n/kaggle/input/stare-dataset/im0340.ppm\n/kaggle/input/stare-dataset/im0198.ppm\n/kaggle/input/stare-dataset/im0136.ppm\n/kaggle/input/stare-dataset/im0174.ppm\n/kaggle/input/stare-dataset/im0354.ppm\n/kaggle/input/stare-dataset/im0116.ppm\n/kaggle/input/stare-dataset/im0314.ppm\n/kaggle/input/stare-dataset/im0306.ppm\n/kaggle/input/stare-dataset/im0200.ppm\n/kaggle/input/stare-dataset/im0166.ppm\n/kaggle/input/stare-dataset/im0265.ppm\n/kaggle/input/stare-dataset/im0184.ppm\n/kaggle/input/stare-dataset/im0156.ppm\n/kaggle/input/stare-dataset/im0239.ppm\n/kaggle/input/stare-dataset/im0260.ppm\n/kaggle/input/stare-dataset/im0114.ppm\n/kaggle/input/stare-dataset/im0066.ppm\n/kaggle/input/stare-dataset/im0059.ppm\n/kaggle/input/stare-dataset/im0121.ppm\n/kaggle/input/stare-dataset/im0320.ppm\n/kaggle/input/stare-dataset/im0130.ppm\n/kaggle/input/stare-dataset/im0092.ppm\n/kaggle/input/stare-dataset/im0039.ppm\n/kaggle/input/stare-dataset/im0398.ppm\n/kaggle/input/stare-dataset/im0213.ppm\n/kaggle/input/stare-dataset/im0402.ppm\n/kaggle/input/stare-dataset/im0180.ppm\n/kaggle/input/stare-dataset/im0264.ppm\n/kaggle/input/stare-dataset/im0327.ppm\n/kaggle/input/stare-dataset/im0268.ppm\n/kaggle/input/stare-dataset/im0319.ppm\n/kaggle/input/stare-dataset/im0189.ppm\n/kaggle/input/stare-dataset/im0291.ppm\n/kaggle/input/stare-dataset/im0329.ppm\n/kaggle/input/stare-dataset/im0382.ppm\n/kaggle/input/stare-dataset/im0035.ppm\n/kaggle/input/stare-dataset/im0370.ppm\n/kaggle/input/stare-dataset/im0188.ppm\n/kaggle/input/stare-dataset/im0140.ppm\n/kaggle/input/stare-dataset/im0243.ppm\n/kaggle/input/stare-dataset/im0040.ppm\n/kaggle/input/stare-dataset/im0155.ppm\n/kaggle/input/stare-dataset/im0272.ppm\n/kaggle/input/stare-dataset/im0353.ppm\n/kaggle/input/stare-dataset/im0055.ppm\n/kaggle/input/stare-dataset/im0335.ppm\n/kaggle/input/stare-dataset/im0179.ppm\n/kaggle/input/stare-dataset/im0067.ppm\n/kaggle/input/stare-dataset/im0376.ppm\n/kaggle/input/stare-dataset/im0270.ppm\n/kaggle/input/stare-dataset/im0099.ppm\n/kaggle/input/stare-dataset/im0073.ppm\n/kaggle/input/stare-dataset/im0311.ppm\n/kaggle/input/stare-dataset/im0172.ppm\n/kaggle/input/stare-dataset/im0312.ppm\n/kaggle/input/stare-dataset/im0248.ppm\n/kaggle/input/stare-dataset/im0197.ppm\n/kaggle/input/stare-dataset/im0098.ppm\n/kaggle/input/stare-dataset/im0218.ppm\n/kaggle/input/stare-dataset/im0102.ppm\n/kaggle/input/stare-dataset/im0212.ppm\n/kaggle/input/stare-dataset/im0090.ppm\n/kaggle/input/stare-dataset/im0388.ppm\n/kaggle/input/stare-dataset/im0147.ppm\n/kaggle/input/stare-dataset/im0001.ppm\n/kaggle/input/chase-db1/Images/Image_12L.jpg\n/kaggle/input/chase-db1/Images/Image_06R.jpg\n/kaggle/input/chase-db1/Images/Image_11R.jpg\n/kaggle/input/chase-db1/Images/Image_14R.jpg\n/kaggle/input/chase-db1/Images/Image_03R.jpg\n/kaggle/input/chase-db1/Images/Image_01R.jpg\n/kaggle/input/chase-db1/Images/Image_05L.jpg\n/kaggle/input/chase-db1/Images/Image_02L.jpg\n/kaggle/input/chase-db1/Images/Image_06L.jpg\n/kaggle/input/chase-db1/Images/Image_09L.jpg\n/kaggle/input/chase-db1/Images/Image_04R.jpg\n/kaggle/input/chase-db1/Images/Image_10L.jpg\n/kaggle/input/chase-db1/Images/Image_08R.jpg\n/kaggle/input/chase-db1/Images/Image_13L.jpg\n/kaggle/input/chase-db1/Images/Image_12R.jpg\n/kaggle/input/chase-db1/Images/Image_10R.jpg\n/kaggle/input/chase-db1/Images/Image_11L.jpg\n/kaggle/input/chase-db1/Images/Image_13R.jpg\n/kaggle/input/chase-db1/Images/Image_07L.jpg\n/kaggle/input/chase-db1/Images/Image_01L.jpg\n/kaggle/input/chase-db1/Images/Image_07R.jpg\n/kaggle/input/chase-db1/Images/Image_05R.jpg\n/kaggle/input/chase-db1/Images/Image_04L.jpg\n/kaggle/input/chase-db1/Images/Image_02R.jpg\n/kaggle/input/chase-db1/Images/Image_09R.jpg\n/kaggle/input/chase-db1/Images/Image_08L.jpg\n/kaggle/input/chase-db1/Images/Image_03L.jpg\n/kaggle/input/chase-db1/Images/Image_14L.jpg\n/kaggle/input/chase-db1/Masks/Image_14R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_02L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_13R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_04R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_02R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_12L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_07R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_08L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_07L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_10R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_13L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_14L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_10L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_04L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_12R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_11L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_05R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_11R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_09L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_03R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_06R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_05L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_09R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_01L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_03L_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_08R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_01R_1stHO.png\n/kaggle/input/chase-db1/Masks/Image_06L_1stHO.png\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/34_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/40_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/22_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/27_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/28_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/26_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/38_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/39_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/24_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/33_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/36_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/32_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/25_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/37_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/29_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/23_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/30_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/35_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/31_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/images/21_training.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/23_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/38_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/35_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/30_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/34_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/27_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/29_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/22_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/25_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/33_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/39_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/36_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/21_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/37_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/24_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/32_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/31_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/40_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/26_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/1st_manual/28_manual1.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/31_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/29_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/27_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/24_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/28_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/35_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/38_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/30_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/22_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/25_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/36_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/32_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/40_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/39_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/37_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/26_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/21_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/23_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/33_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/training/mask/34_training_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/10_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/15_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/19_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/05_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/06_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/18_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/01_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/13_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/07_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/16_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/04_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/20_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/11_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/08_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/09_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/17_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/02_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/12_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/14_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/images/03_test.tif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/13_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/09_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/18_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/20_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/14_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/12_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/04_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/16_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/08_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/06_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/01_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/07_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/03_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/05_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/15_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/11_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/02_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/19_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/10_test_mask.gif\n/kaggle/input/drive-digital-retinal-images-for-vessel-extraction/DRIVE/test/mask/17_test_mask.gif\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#pip install LPIPS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:37.095143Z","iopub.execute_input":"2025-05-05T17:46:37.095507Z","iopub.status.idle":"2025-05-05T17:46:37.098583Z","shell.execute_reply.started":"2025-05-05T17:46:37.095486Z","shell.execute_reply":"2025-05-05T17:46:37.097954Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#pip install torch-fidelity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:37.099937Z","iopub.execute_input":"2025-05-05T17:46:37.100138Z","iopub.status.idle":"2025-05-05T17:46:37.111648Z","shell.execute_reply.started":"2025-05-05T17:46:37.100120Z","shell.execute_reply":"2025-05-05T17:46:37.110898Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom PIL import Image\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.utils as vutils\nfrom torchvision.models import inception_v3,Inception_V3_Weights\nfrom torchvision.transforms import Resize\nfrom torch.nn.functional import adaptive_avg_pool2d\nimport torch.nn.utils.spectral_norm as spectral_norm\nfrom scipy import linalg\nfrom scipy.linalg import sqrtm\nfrom PIL import ImageOps\nfrom PIL import ImageEnhance\nfrom torchmetrics.image.kid import KernelInceptionDistance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:37.112865Z","iopub.execute_input":"2025-05-05T17:46:37.113083Z","iopub.status.idle":"2025-05-05T17:46:47.969175Z","shell.execute_reply.started":"2025-05-05T17:46:37.113064Z","shell.execute_reply":"2025-05-05T17:46:47.968310Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class STAREPreprocessor:\n    def __init__(self, input_dir, output_base, resize_size=(256, 256)):\n        \"\"\"\n        Initialize the STARE dataset preprocessor\n        \n        Args:\n            input_dir: Directory containing original STARE .ppm images\n            output_base: Base directory for processed outputs\n            resize_size: Target size for resized images (default: 256x256)\n        \"\"\"\n        self.input_dir = input_dir\n        self.output_base = output_base\n        self.resize_size = resize_size\n        \n        # Output folders\n        self.resize_dir = os.path.join(output_base, \"resized\")\n        self.augment_dir = os.path.join(output_base, \"augmented\")\n        self.enhance_dir = os.path.join(output_base, \"enhanced\")\n        \n        # Create directories\n        os.makedirs(self.resize_dir, exist_ok=True)\n        os.makedirs(self.augment_dir, exist_ok=True)\n        os.makedirs(self.enhance_dir, exist_ok=True)\n        \n        # Data augmentation\n        self.augmentations = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(20),\n            transforms.RandomVerticalFlip(),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n        ])\n    \n    def resize_with_padding(self, img, target_size, fill_color=(0, 0, 0)):\n        \"\"\"\n        Resize image while maintaining aspect ratio and adding padding if needed\n        \n        Args:\n            img: PIL Image\n            target_size: Desired size (width, height)\n            fill_color: Color to use for padding\n            \n        Returns:\n            Resized and padded PIL Image\n        \"\"\"\n        img.thumbnail(target_size, Image.LANCZOS)\n        delta_w = target_size[0] - img.size[0]\n        delta_h = target_size[1] - img.size[1]\n        padding = (\n            delta_w // 2, delta_h // 2,\n            delta_w - (delta_w // 2), delta_h - (delta_h // 2)\n        )\n        return ImageOps.expand(img, padding, fill_color)\n    \n    def enhance_image(self, img):\n        \"\"\"\n        Enhance image with contrast, sharpness, and color adjustments\n        \n        Args:\n            img: PIL Image\n            \n        Returns:\n            Enhanced PIL Image\n        \"\"\"\n        img = ImageEnhance.Contrast(img).enhance(1.5)\n        img = ImageEnhance.Sharpness(img).enhance(2.0)\n        img = ImageEnhance.Color(img).enhance(1.3)\n        return img\n    \n    def process_images(self, num_augmentations=2, save_enhanced=True, save_resized=True):\n        \"\"\"\n        Process all images in the input directory\n        \n        Args:\n            num_augmentations: Number of augmented versions to create per image\n            save_enhanced: Whether to save enhanced versions\n            save_resized: Whether to save resized versions\n        \"\"\"\n        # Get all PPM files\n        image_files = [f for f in os.listdir(self.input_dir) if f.endswith(\".ppm\")]\n        \n        # Process each image\n        for filename in tqdm(image_files, desc=\"Processing images\"):\n            img_path = os.path.join(self.input_dir, filename)\n            image = Image.open(img_path).convert(\"RGB\")\n            \n            # Get base filename without extension\n            base_filename = os.path.splitext(filename)[0]\n            \n            # Resize with padding to maintain aspect ratio\n            resized = self.resize_with_padding(image, self.resize_size)\n            \n            # Save resized image\n            if save_resized:\n                resized.save(f\"{self.resize_dir}/{base_filename}.png\")\n            \n            # Save original in augmented folder\n            resized.save(f\"{self.augment_dir}/{base_filename}.png\")\n            \n            # Create and save augmented versions\n            for i in range(num_augmentations):\n                augmented = self.augmentations(resized)\n                augmented.save(f\"{self.augment_dir}/{base_filename}_aug{i}.png\")\n            \n            # Create and save enhanced version\n            if save_enhanced:\n                enhanced = self.enhance_image(resized)\n                enhanced.save(f\"{self.enhance_dir}/{base_filename}_enhanced.png\")\n                \n                # Also create augmented enhanced versions\n                for i in range(num_augmentations):\n                    aug_enhanced = self.augmentations(enhanced)\n                    aug_enhanced.save(f\"{self.enhance_dir}/{base_filename}_enhanced_aug{i}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:47.970163Z","iopub.execute_input":"2025-05-05T17:46:47.970678Z","iopub.status.idle":"2025-05-05T17:46:47.980774Z","shell.execute_reply.started":"2025-05-05T17:46:47.970647Z","shell.execute_reply":"2025-05-05T17:46:47.979775Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class STAREDataset(Dataset):\n    def __init__(self, data_dir, transform=None, use_augmented=True, use_enhanced=True):\n        \"\"\"\n        PyTorch Dataset for processed STARE images\n        \n        Args:\n            data_dir: Base directory containing processed images\n            transform: Additional transforms to apply\n            use_augmented: Whether to include augmented images\n            use_enhanced: Whether to include enhanced images\n        \"\"\"\n        self.data_dir = data_dir\n        self.transform = transform\n        \n        # Collect image paths\n        self.image_paths = []\n        \n        # Add resized images\n        resize_dir = os.path.join(data_dir, \"resized\")\n        if os.path.exists(resize_dir):\n            self.image_paths.extend([\n                os.path.join(resize_dir, f) for f in os.listdir(resize_dir)\n                if f.endswith(('.png', '.jpg'))\n            ])\n        \n        # Add augmented images\n        if use_augmented:\n            augment_dir = os.path.join(data_dir, \"augmented\")\n            if os.path.exists(augment_dir):\n                self.image_paths.extend([\n                    os.path.join(augment_dir, f) for f in os.listdir(augment_dir)\n                    if f.endswith(('.png', '.jpg'))\n                ])\n        \n        # Add enhanced images\n        if use_enhanced:\n            enhance_dir = os.path.join(data_dir, \"enhanced\")\n            if os.path.exists(enhance_dir):\n                self.image_paths.extend([\n                    os.path.join(enhance_dir, f) for f in os.listdir(enhance_dir)\n                    if f.endswith(('.png', '.jpg'))\n                ])\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:47.981803Z","iopub.execute_input":"2025-05-05T17:46:47.982147Z","iopub.status.idle":"2025-05-05T17:46:47.999339Z","shell.execute_reply.started":"2025-05-05T17:46:47.982112Z","shell.execute_reply":"2025-05-05T17:46:47.998571Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"input_dir = \"/kaggle/input/stare-dataset\"\noutput_base = \"/kaggle/working/stare_prepared\"\n# Create and run preprocessor\npreprocessor = STAREPreprocessor(input_dir, output_base, resize_size=(256, 256))\npreprocessor.process_images(num_augmentations=3, save_enhanced=True)\n    \n# Create dataset with normalization transform\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\ndataset = STAREDataset(output_base, transform=transform)\n# Create dataloader\nbatch_size = 16\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:46:47.999977Z","iopub.execute_input":"2025-05-05T17:46:48.000165Z","iopub.status.idle":"2025-05-05T17:48:28.868380Z","shell.execute_reply.started":"2025-05-05T17:46:48.000148Z","shell.execute_reply":"2025-05-05T17:48:28.867644Z"}},"outputs":[{"name":"stderr","text":"Processing images: 100%|██████████| 397/397 [01:40<00:00,  3.94it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"latent_dim = 100 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.869112Z","iopub.execute_input":"2025-05-05T17:48:28.869399Z","iopub.status.idle":"2025-05-05T17:48:28.872877Z","shell.execute_reply.started":"2025-05-05T17:48:28.869365Z","shell.execute_reply":"2025-05-05T17:48:28.872217Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DCGANGenerator(nn.Module):\n    def __init__(self, z_dim=100, channels=3, features_g=64):\n        \"\"\"\n        DCGAN Generator implementation\n        \n        Args:\n            z_dim: Size of the noise vector\n            channels: Number of output channels (3 for RGB)\n            features_g: Size factor for generator (higher = more capacity)\n        \"\"\"\n        super(DCGANGenerator, self).__init__()\n        self.z_dim = z_dim\n        \n        # Starting with 1x1 projection from noise\n        self.project = nn.Linear(z_dim, features_g * 8 * 4 * 4)\n        \n        # Main convolutional blocks for upsampling\n        self.main = nn.Sequential(\n            # Block 1: 4x4 -> 8x8\n            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 4),\n            nn.ReLU(True),\n            \n            # Block 2: 8x8 -> 16x16\n            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 2),\n            nn.ReLU(True),\n            \n            # Block 3: 16x16 -> 32x32\n            nn.ConvTranspose2d(features_g * 2, features_g, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g),\n            nn.ReLU(True),\n            \n            # Block 4: 32x32 -> 64x64\n            nn.ConvTranspose2d(features_g, features_g // 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g // 2),\n            nn.ReLU(True),\n            \n            # Block 5: 64x64 -> 128x128\n            nn.ConvTranspose2d(features_g // 2, features_g // 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g // 4),\n            nn.ReLU(True),\n            \n            # Output layer: 128x128 -> 256x256\n            nn.ConvTranspose2d(features_g // 4, channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()  # Output range: [-1, 1]\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n        elif classname.find('Linear') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, z):\n        # Project and reshape\n        x = self.project(z)\n        x = x.view(-1, 8 * self.z_dim // 25, 4, 4)  # Shape becomes (batch_size, features_g*8, 4, 4)\n        # Pass through main convolutional blocks\n        return self.main(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.875594Z","iopub.execute_input":"2025-05-05T17:48:28.875810Z","iopub.status.idle":"2025-05-05T17:48:28.888976Z","shell.execute_reply.started":"2025-05-05T17:48:28.875792Z","shell.execute_reply":"2025-05-05T17:48:28.888141Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class DCGANDiscriminator(nn.Module):\n    def __init__(self, channels=3, features_d=64):\n        \"\"\"\n        DCGAN Discriminator implementation\n        \n        Args:\n            channels: Number of input channels (3 for RGB)\n            features_d: Size factor for discriminator (higher = more capacity)\n        \"\"\"\n        super(DCGANDiscriminator, self).__init__()\n        \n        self.main = nn.Sequential(\n            # Input: 256x256x3\n            nn.Conv2d(channels, features_d, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 128x128 -> 64x64\n            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 64x64 -> 32x32\n            nn.Conv2d(features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 32x32 -> 16x16\n            nn.Conv2d(features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 16x16 -> 8x8\n            nn.Conv2d(features_d * 8, features_d * 16, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 8x8 -> 4x4\n            nn.Conv2d(features_d * 16, features_d * 32, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 32),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 4x4 -> 1x1\n            nn.Conv2d(features_d * 32, 1, kernel_size=4, stride=1, padding=0, bias=False)\n            # No sigmoid here - we'll use BCEWithLogitsLoss\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, x):\n        return self.main(x).view(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.890147Z","iopub.execute_input":"2025-05-05T17:48:28.890380Z","iopub.status.idle":"2025-05-05T17:48:28.903729Z","shell.execute_reply.started":"2025-05-05T17:48:28.890361Z","shell.execute_reply":"2025-05-05T17:48:28.903042Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# For 256x256 images\nclass DCGAN256Generator(nn.Module):\n    def __init__(self, z_dim=100, channels=3, features_g=64):\n        super(DCGAN256Generator, self).__init__()\n        self.z_dim = z_dim\n        \n        # Starting with 1x1 projection from noise\n        self.project = nn.Linear(z_dim, features_g * 16 * 4 * 4)\n        \n        # Main convolutional blocks for upsampling\n        self.main = nn.Sequential(\n            # Block 1: 4x4 -> 8x8\n            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 8),\n            nn.ReLU(True),\n            \n            # Block 2: 8x8 -> 16x16\n            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 4),\n            nn.ReLU(True),\n            \n            # Block 3: 16x16 -> 32x32\n            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 2),\n            nn.ReLU(True),\n            \n            # Block 4: 32x32 -> 64x64\n            nn.ConvTranspose2d(features_g * 2, features_g, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g),\n            nn.ReLU(True),\n            \n            # Block 5: 64x64 -> 128x128\n            nn.ConvTranspose2d(features_g, features_g // 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g // 2),\n            nn.ReLU(True),\n            \n            # Output layer: 128x128 -> 256x256\n            nn.ConvTranspose2d(features_g // 2, channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()  # Output range: [-1, 1]\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n        elif classname.find('Linear') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, z):\n        # Reshape noise to (batch_size, 1024, 4, 4)\n        x = z.view(-1, 1024, 4, 4)  # Fixed channel count\n        return self.main(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.904467Z","iopub.execute_input":"2025-05-05T17:48:28.904774Z","iopub.status.idle":"2025-05-05T17:48:28.922595Z","shell.execute_reply.started":"2025-05-05T17:48:28.904741Z","shell.execute_reply":"2025-05-05T17:48:28.921801Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n# For 256x256 images - Discriminator \nclass DCGAN256Discriminator(nn.Module):\n    def __init__(self, channels=3, features_d=64):\n        super(DCGAN256Discriminator, self).__init__()\n        \n        self.main = nn.Sequential(\n            # Input: 256x256x3\n            nn.Conv2d(channels, features_d, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 128x128 -> 64x64\n            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 64x64 -> 32x32\n            nn.Conv2d(features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 32x32 -> 16x16\n            nn.Conv2d(features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 16x16 -> 8x8\n            nn.Conv2d(features_d * 8, features_d * 16, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 8x8 -> 4x4\n            nn.Conv2d(features_d * 16, features_d * 32, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_d * 32),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 4x4 -> 1x1\n            nn.Conv2d(features_d * 32, 1, kernel_size=4, stride=1, padding=0, bias=False)\n            # No sigmoid here - we'll use BCEWithLogitsLoss\n        )\n        \n        # Initialize weights\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, x):\n        return self.main(x).view(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.923443Z","iopub.execute_input":"2025-05-05T17:48:28.923731Z","iopub.status.idle":"2025-05-05T17:48:28.939421Z","shell.execute_reply.started":"2025-05-05T17:48:28.923699Z","shell.execute_reply":"2025-05-05T17:48:28.938627Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# For 256x256 STARE dataset\nclass DCGAN256STARE(nn.Module):\n    def __init__(self, z_dim=100, channels=3, features_g=64):\n        super(DCGAN256STARE, self).__init__()\n        self.z_dim = z_dim\n        self.features_g=features_g\n        # Starting with projection from noise\n        self.project = nn.Sequential(\n            nn.Linear(z_dim, features_g * 16 * 4 * 4),\n            nn.BatchNorm1d(features_g * 16 * 4 * 4),\n            nn.ReLU(True)\n        )\n        \n        # Main convolutional blocks for upsampling to 256x256\n        self.main = nn.Sequential(\n            # Block 1: 4x4 -> 8x8\n            nn.ConvTranspose2d(features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 8),\n            nn.ReLU(True),\n            \n            # Block 2: 8x8 -> 16x16\n            nn.ConvTranspose2d(features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 4),\n            nn.ReLU(True),\n            \n            # Block 3: 16x16 -> 32x32\n            nn.ConvTranspose2d(features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g * 2),\n            nn.ReLU(True),\n            \n            # Block 4: 32x32 -> 64x64\n            nn.ConvTranspose2d(features_g * 2, features_g, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g),\n            nn.ReLU(True),\n            \n            # Block 5: 64x64 -> 128x128\n            nn.ConvTranspose2d(features_g, features_g // 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(features_g // 2),\n            nn.ReLU(True),\n            \n            # Output layer: 128x128 -> 256x256\n            nn.ConvTranspose2d(features_g // 2, channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()  # Output range: [-1, 1]\n        )\n        \n        # Initialize weights using DCGAN paper recommendation\n        self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n        elif classname.find('Linear') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, z):\n        x = self.project(z)   # Now this will work!\n        x = x.view(-1, 1024, 4, 4)\n        return self.main(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.940200Z","iopub.execute_input":"2025-05-05T17:48:28.940496Z","iopub.status.idle":"2025-05-05T17:48:28.953946Z","shell.execute_reply.started":"2025-05-05T17:48:28.940467Z","shell.execute_reply":"2025-05-05T17:48:28.953325Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# For 256x256 STARE dataset - Discriminator with spectral normalization\nclass STAREDiscriminator(nn.Module):\n    def __init__(self, channels=3, features_d=64, use_spectral_norm=True):\n        super(STAREDiscriminator, self).__init__()\n        \n        # Choose between regular Conv and Spectral Normalized Conv\n        if use_spectral_norm:\n            conv_layer = lambda in_c, out_c, k, s, p: nn.utils.spectral_norm(\n                nn.Conv2d(in_c, out_c, kernel_size=k, stride=s, padding=p, bias=False)\n            )\n        else:\n            conv_layer = lambda in_c, out_c, k, s, p: nn.Conv2d(\n                in_c, out_c, kernel_size=k, stride=s, padding=p, bias=False\n            )\n        \n        self.main = nn.Sequential(\n            # Input: 256x256x3\n            conv_layer(channels, features_d, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 128x128 -> 64x64\n            conv_layer(features_d, features_d * 2, 4, 2, 1),\n            nn.BatchNorm2d(features_d * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 64x64 -> 32x32\n            conv_layer(features_d * 2, features_d * 4, 4, 2, 1),\n            nn.BatchNorm2d(features_d * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 32x32 -> 16x16\n            conv_layer(features_d * 4, features_d * 8, 4, 2, 1),\n            nn.BatchNorm2d(features_d * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 16x16 -> 8x8\n            conv_layer(features_d * 8, features_d * 16, 4, 2, 1),\n            nn.BatchNorm2d(features_d * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 8x8 -> 4x4\n            conv_layer(features_d * 16, features_d * 16, 4, 2, 1),\n            nn.BatchNorm2d(features_d * 16),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # 4x4 -> 1x1\n            conv_layer(features_d * 16, 1, 4, 1, 0)\n        )\n        \n        # Initialize weights if not using spectral norm\n        if not use_spectral_norm:\n            self.apply(self._init_weights)\n        \n    def _init_weights(self, m):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n    \n    def forward(self, x):\n        return self.main(x).view(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.954669Z","iopub.execute_input":"2025-05-05T17:48:28.954890Z","iopub.status.idle":"2025-05-05T17:48:28.969158Z","shell.execute_reply.started":"2025-05-05T17:48:28.954871Z","shell.execute_reply":"2025-05-05T17:48:28.968606Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Example of how to use these models:\ndef create_models(z_dim=100, image_channels=3, features_g=64, features_d=64):\n    \"\"\"\n    Create DCGAN models for a 256x256 STARE dataset\n    \n    Args:\n        z_dim: Size of noise vector\n        image_channels: Number of channels in the image (3 for RGB)\n        features_g: Base feature size for generator\n        features_d: Base feature size for discriminator\n        \n    Returns:\n        generator, discriminator models\n    \"\"\"\n    generator = DCGAN256STARE(z_dim=z_dim, channels=image_channels, features_g=features_g)\n    discriminator = STAREDiscriminator(channels=image_channels, features_d=features_d, use_spectral_norm=True)\n    \n    return generator, discriminator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.969930Z","iopub.execute_input":"2025-05-05T17:48:28.970195Z","iopub.status.idle":"2025-05-05T17:48:28.983818Z","shell.execute_reply.started":"2025-05-05T17:48:28.970175Z","shell.execute_reply":"2025-05-05T17:48:28.983062Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, z_dim=100, num_images=16):\n    model.eval()\n    with torch.no_grad():\n        z = torch.randn(num_images, z_dim, device=device)\n        fake_images = model(z).detach().cpu()\n        grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n        plt.figure(figsize=(8, 8))\n        plt.imshow(np.transpose(grid, (1, 2, 0)))\n        plt.axis(\"off\")\n        plt.title(f\"Generated Images - Epoch {epoch}\")\n        plt.show()\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.984588Z","iopub.execute_input":"2025-05-05T17:48:28.984839Z","iopub.status.idle":"2025-05-05T17:48:28.998048Z","shell.execute_reply.started":"2025-05-05T17:48:28.984808Z","shell.execute_reply":"2025-05-05T17:48:28.997322Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.utils as vutils\nfrom lpips import LPIPS\nimport numpy as np\nfrom tqdm import tqdm\n\n# Create directories to save generated images\nos.makedirs(\"generated_images\", exist_ok=True)\nos.makedirs(\"real_images\", exist_ok=True)\n\ndef train_dcgan(G, D, dataloader, z_dim=100, epochs=100, device=\"cuda\", \n                lr=0.0002, betas=(0.5, 0.999), save_interval=1):\n    \"\"\"\n    Training function for DCGAN on STARE dataset\n    \n    Args:\n        G: Generator model\n        D: Discriminator model\n        dataloader: DataLoader with STARE images\n        z_dim: Dimensionality of noise vector\n        epochs: Number of training epochs\n        device: Device to train on ('cuda' or 'cpu')\n        lr: Learning rate\n        betas: Adam optimizer betas\n        save_interval: How often to save images and evaluate metrics\n    \"\"\"\n    # Loss function - using BCE with logits for numerical stability\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # Label smoothing - use 0.9 for real labels instead of 1.0\n    real_label_val = 0.9\n    fake_label_val = 0.0\n    \n    # Optimizers with proper hyperparameters for DCGAN\n    optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=betas)\n    optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=betas)\n    \n    # Create a dictionary to store metrics\n    metrics = {\n        'epochs': [],\n        'fid': [],\n        'kid': [],\n        'lpips': [],\n        'ssim': [],\n        'psnr': [],\n        'diversity': [],\n        'd_losses': [],\n        'g_losses': []\n    }\n    \n    # Fixed noise for visualization\n    torch.manual_seed(42)\n    fixed_noise = torch.randn(64, z_dim, device=device)\n    \n    # Loss tracking\n    D_losses = []\n    G_losses = []\n    \n    # Initialize lpips model for evaluation\n    lpips_model = LPIPS(net='alex').to(device).eval()\n    \n    # Training loop\n    for epoch in range(epochs):\n        epoch_d_loss = 0.0\n        epoch_g_loss = 0.0\n        \n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for batch_idx, real_images in enumerate(progress_bar):\n            # Get batch size (might be smaller for last batch)\n            batch_size = real_images.size(0)\n            real_images = real_images.to(device)\n            \n            # ===== Train Discriminator =====\n            # Zero gradients\n            optimizer_D.zero_grad()\n            \n            # Generate labels with noise for label smoothing\n            real_labels = torch.full((batch_size, 1), real_label_val, device=device)\n            # Add some noise to labels for stabilization\n            real_labels = real_labels - 0.05 * torch.rand(real_labels.size(), device=device)\n            \n            # Forward pass real batch through D\n            outputs_real = D(real_images)\n            \n            # Calculate loss on real images\n            loss_real = criterion(outputs_real, real_labels)\n            \n            # Generate fake images batch\n            noise = torch.randn(batch_size, z_dim, device=device)\n            fake_images = G(noise)\n            \n            # Classify fake images\n            fake_labels = torch.full((batch_size, 1), fake_label_val, device=device)\n            outputs_fake = D(fake_images.detach())  # Detach to avoid training G\n            \n            # Calculate loss on fake images\n            loss_fake = criterion(outputs_fake, fake_labels)\n            \n            # Combined D loss and backprop\n            loss_D = loss_real + loss_fake\n            loss_D.backward()\n            optimizer_D.step()\n            \n            # ===== Train Generator =====\n            # Zero G gradients\n            optimizer_G.zero_grad()\n            \n            # Generate another batch of fake images (some implementations reuse, but fresh batch can help)\n            noise = torch.randn(batch_size, z_dim, device=device)\n            fake_images = G(noise)\n            \n            # Try to fool the discriminator\n            outputs = D(fake_images)\n            \n            # G wants D to think these are real\n            loss_G = criterion(outputs, real_labels)\n            \n            # Backprop and update G\n            loss_G.backward()\n            optimizer_G.step()\n            \n            # Update statistics\n            epoch_d_loss += loss_D.item()\n            epoch_g_loss += loss_G.item()\n            \n            # Update the progress bar\n            progress_bar.set_postfix({\n                'D_loss': f\"{loss_D.item():.4f}\", \n                'G_loss': f\"{loss_G.item():.4f}\"\n            })\n        \n        # Calculate average epoch losses\n        avg_d_loss = epoch_d_loss / len(dataloader)\n        avg_g_loss = epoch_g_loss / len(dataloader)\n        D_losses.append(avg_d_loss)\n        G_losses.append(avg_g_loss)\n        \n        print(f\"Epoch {epoch+1}/{epochs} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n        \n        # Generate and save images every save_interval epochs\n        if epoch % save_interval == 0 or epoch == epochs - 1:\n            # Generate fake images for evaluation\n            G.eval()\n            with torch.no_grad():\n                fake_samples = G(fixed_noise).detach().cpu()\n            G.train()\n            \n            # Adjust image count to prevent ValueError on smaller batches\n            real_batch_size = min(64, real_images.size(0))\n            real_images_vis = real_images[:real_batch_size].detach().cpu()\n            fake_samples_vis = fake_samples[:real_batch_size]\n            \n            # Save generated & real images\n            vutils.save_image(fake_samples_vis, f\"generated_images/epoch_{epoch+1:03d}.png\", \n                             normalize=True, nrow=8)\n            vutils.save_image(real_images_vis, f\"real_images/epoch_{epoch+1:03d}.png\", \n                             normalize=True, nrow=8)\n            \n            # Evaluate metrics and update tracking\n            fid_score, kid_score, lpips_avg, ssim_avg, psnr_avg, diversity_avg = evaluate_metrics(\n                real_images_vis, fake_samples_vis, epoch, device)\n            \n            # Record metrics\n            update_metrics(metrics, epoch, fid_score, kid_score, lpips_avg, ssim_avg, \n                          psnr_avg, diversity_avg, avg_d_loss, avg_g_loss)\n            \n            # Save model checkpoints\n            if (epoch + 1) % 5 == 0:\n                torch.save(G.state_dict(), f\"model_checkpoints/generator_{epoch+1}.pth\")\n\n    \n    return G, D, metrics","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:28.998756Z","iopub.execute_input":"2025-05-05T17:48:28.998937Z","iopub.status.idle":"2025-05-05T17:48:29.016299Z","shell.execute_reply.started":"2025-05-05T17:48:28.998920Z","shell.execute_reply":"2025-05-05T17:48:29.015433Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def update_metrics(metrics_dict, epoch, fid, kid, lpips, ssim, psnr, diversity, d_loss, g_loss):\n    \"\"\"Update metrics dictionary with values from current epoch\"\"\"\n    metrics_dict['epochs'].append(epoch + 1)  # 1-indexed for plotting\n    metrics_dict['fid'].append(fid)\n    metrics_dict['kid'].append(kid)\n    metrics_dict['lpips'].append(lpips)\n    metrics_dict['ssim'].append(ssim)\n    metrics_dict['psnr'].append(psnr)\n    metrics_dict['diversity'].append(diversity)\n    metrics_dict['d_losses'].append(d_loss)\n    metrics_dict['g_losses'].append(g_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:29.017079Z","iopub.execute_input":"2025-05-05T17:48:29.017341Z","iopub.status.idle":"2025-05-05T17:48:29.026403Z","shell.execute_reply.started":"2025-05-05T17:48:29.017320Z","shell.execute_reply":"2025-05-05T17:48:29.025652Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def evaluate_metrics(real_images, fake_images, epoch, device):\n    \"\"\"\n    Evaluate image quality metrics comparing real and generated images\n    \n    Args:\n        real_images: Batch of real images\n        fake_images: Batch of generated images\n        epoch: Current epoch number\n        device: Device to perform calculations on\n        \n    Returns:\n        Tuple of (fid_score, kid_score, lpips_avg, ssim_avg, psnr_avg, diversity_avg)\n    \"\"\"\n    from torchvision.models import inception_v3\n    from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n    from skimage.metrics import structural_similarity as ssim\n    from skimage.metrics import peak_signal_noise_ratio as psnr\n    from torchmetrics.image.kid import KernelInceptionDistance\n    import numpy as np\n    from scipy import linalg\n    import torchvision.transforms as transforms\n\n    lpips_model = LPIPS(net='alex').to(device).eval()\n    inception = inception_v3(pretrained=True, transform_input=False).to(device).eval()\n    resize_norm = Compose([\n        Resize((299, 299)),\n        ToTensor(),\n        Normalize([0.5]*3, [0.5]*3)\n    ])\n\n    # --- FID ---\n    def get_activations(imgs):\n        # Convert tensor images to PIL for transform\n        if torch.is_tensor(imgs[0]):\n            imgs = [transforms.ToPILImage()(img) for img in imgs]\n            \n        imgs = torch.stack([resize_norm(i) for i in imgs]).to(device)\n        with torch.no_grad():\n            # Get features before the final classification layer\n            inception.aux_logits = False\n            features = inception(imgs)\n        return features.cpu().numpy()\n\n    # Convert image formats for metrics calculations\n    real_np = real_images.permute(0, 2, 3, 1).cpu().numpy()\n    fake_np = fake_images.permute(0, 2, 3, 1).cpu().numpy()\n\n    # Calculate Inception features\n    act_real = get_activations([transforms.ToPILImage()(img) for img in real_images])\n    act_fake = get_activations([transforms.ToPILImage()(img) for img in fake_images])\n\n    # Calculate FID\n    mu1, sigma1 = act_real.mean(axis=0), np.cov(act_real, rowvar=False)\n    mu2, sigma2 = act_fake.mean(axis=0), np.cov(act_fake, rowvar=False)\n    diff = mu1 - mu2\n    \n    # Calculate sqrt of product between cov\n    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n    if np.iscomplexobj(covmean): \n        covmean = covmean.real\n    \n    fid_score = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n\n    # --- KID ---\n    # Use smaller subset size if fewer images available\n    subset_size = min(len(real_images), len(fake_images), 10)\n    kid_metric = KernelInceptionDistance(subset_size=subset_size).to(device)\n    \n    # Convert images to uint8 format for KID\n    real_uint8 = (real_images * 255).to(torch.uint8).to(device)\n    fake_uint8 = (fake_images * 255).to(torch.uint8).to(device)\n    \n    # Update KID metric with images\n    kid_metric.update(real_uint8, real=True)\n    kid_metric.update(fake_uint8, real=False)\n    \n    # Compute KID\n    kid_mean, kid_std = kid_metric.compute()\n    kid_score = kid_mean.item()\n\n    # --- LPIPS & Diversity ---\n    lpips_scores = []\n    diversity_scores = []\n    num_eval = min(len(real_images), len(fake_images), 10)\n    imgs = fake_images[:num_eval]\n    \n    for i in range(len(imgs)):\n        for j in range(i+1, len(imgs)):\n            # Scale images to [-1, 1] for LPIPS\n            f1 = (imgs[i].unsqueeze(0).to(device) - 0.5) * 2\n            f2 = (imgs[j].unsqueeze(0).to(device) - 0.5) * 2\n            score = lpips_model(f1, f2).item()\n            diversity_scores.append(score)\n    \n    for real, fake in zip(real_images[:num_eval], fake_images[:num_eval]):\n        # Scale images to [-1, 1] for LPIPS\n        r = (real.unsqueeze(0).to(device) - 0.5) * 2\n        f = (fake.unsqueeze(0).to(device) - 0.5) * 2\n        lpips_scores.append(lpips_model(r, f).item())\n\n    # --- SSIM & PSNR ---\n    ssim_scores = []\n    psnr_scores = []\n    for r, f in zip(real_np[:num_eval], fake_np[:num_eval]):\n        r_img = (r * 255).astype(np.uint8)\n        f_img = (f * 255).astype(np.uint8)\n        ssim_scores.append(ssim(r_img, f_img, channel_axis=-1))\n        psnr_scores.append(psnr(r_img, f_img))\n\n    # Averages\n    lpips_avg = np.mean(lpips_scores)\n    ssim_avg = np.mean(ssim_scores)\n    psnr_avg = np.mean(psnr_scores)\n    diversity_avg = np.mean(diversity_scores) if diversity_scores else 0.0\n\n    print(f\"\\n📊 Epoch {epoch + 1} Metrics:\")\n    print(f\"✅ FID:       {fid_score:.4f}\")\n    print(f\"✅ KID:       {kid_score:.4f}\")\n    print(f\"✅ LPIPS:     {lpips_avg:.4f}\")\n    print(f\"✅ SSIM:      {ssim_avg:.4f}\")\n    print(f\"✅ PSNR:      {psnr_avg:.4f}\")\n    print(f\"✅ Diversity: {diversity_avg:.4f}\")\n\n    return fid_score, kid_score, lpips_avg, ssim_avg, psnr_avg, diversity_avg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:29.027191Z","iopub.execute_input":"2025-05-05T17:48:29.027546Z","iopub.status.idle":"2025-05-05T17:48:29.044125Z","shell.execute_reply.started":"2025-05-05T17:48:29.027511Z","shell.execute_reply":"2025-05-05T17:48:29.043493Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def plot_training_metrics(metrics):\n    \"\"\"Plot training metrics over time\"\"\"\n    import matplotlib.pyplot as plt\n    \n    # Create directory for plots\n    os.makedirs(\"training_plots\", exist_ok=True)\n    \n    # Plot FID over epochs\n    plt.figure(figsize=(10, 6))\n    plt.plot(metrics['epochs'], metrics['fid'], marker='o', color='green')\n    plt.title('FID Score Over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel('FID Score (lower is better)')\n    plt.grid(True)\n    plt.savefig('training_plots/fid.png')\n    plt.close()\n    \n    # Plot KID over epochs\n    plt.figure(figsize=(10, 6))\n    plt.plot(metrics['epochs'], metrics['kid'], marker='o', color='green')\n    plt.title('KID Score Over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel('KID Score (lower is better)')\n    plt.grid(True)\n    plt.savefig('training_plots/kid.png')\n    plt.close()\n    \n    # Plot LPIPS over epochs\n    plt.figure(figsize=(10, 6))\n    plt.plot(metrics['epochs'], metrics['lpips'], marker='o', color='green')\n    plt.title('LPIPS Score Over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel('LPIPS (lower is better)')\n    plt.grid(True)\n    plt.savefig('training_plots/lpips.png')\n    plt.close()\n    \n    # Plot losses\n    plt.figure(figsize=(10, 6))\n    plt.plot(metrics['epochs'], metrics['d_losses'], label='Discriminator Loss')\n    plt.plot(metrics['epochs'], metrics['g_losses'], label='Generator Loss')\n    plt.title('Training Losses')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.savefig('training_plots/losses.png')\n    plt.close()\n    \n    # Plot diversity\n    plt.figure(figsize=(10, 6))\n    plt.plot(metrics['epochs'], metrics['diversity'], marker='o', color='green')\n    plt.title('Image Diversity Over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel('Diversity Score (higher is better)')\n    plt.grid(True)\n    plt.savefig('training_plots/diversity.png')\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:29.044903Z","iopub.execute_input":"2025-05-05T17:48:29.045184Z","iopub.status.idle":"2025-05-05T17:48:29.060160Z","shell.execute_reply.started":"2025-05-05T17:48:29.045156Z","shell.execute_reply":"2025-05-05T17:48:29.059334Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch.nn as nn\nz_dim = 100\nepochs = 100\nbatch_size = 32\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n# Get models\nG, D = create_models(z_dim=z_dim)\nG.to(device)\nD.to(device)    \n# Create directory for model checkpoints\nos.makedirs(\"model_checkpoints\", exist_ok=True)\nG_trained, D_trained, metrics = train_dcgan(G, D, dataloader, z_dim, epochs, device)    \n# Plot metrics after training\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:29.060967Z","iopub.execute_input":"2025-05-05T17:48:29.061279Z","iopub.status.idle":"2025-05-05T20:23:01.788904Z","shell.execute_reply.started":"2025-05-05T17:48:29.061223Z","shell.execute_reply":"2025-05-05T20:23:01.788010Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n100%|██████████| 233M/233M [00:01<00:00, 198MB/s] \n/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n","output_type":"stream"},{"name":"stdout","text":"Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100: 100%|██████████| 224/224 [01:22<00:00,  2.70it/s, D_loss=0.6145, G_loss=3.2197] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100 | D Loss: 0.7703 | G Loss: 6.0847\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 205MB/s] \n/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `Kernel Inception Distance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n  warnings.warn(*args, **kwargs)  # noqa: B028\nDownloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n100%|██████████| 91.2M/91.2M [00:00<00:00, 271MB/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Epoch 1 Metrics:\n✅ FID:       1765.9406\n✅ KID:       0.4753\n✅ LPIPS:     0.7568\n✅ SSIM:      0.0167\n✅ PSNR:      4.3124\n✅ Diversity: 0.2047\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100: 100%|██████████| 224/224 [01:28<00:00,  2.53it/s, D_loss=0.4578, G_loss=3.9289] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/100 | D Loss: 0.9164 | G Loss: 3.4799\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 2 Metrics:\n✅ FID:       1931.4250\n✅ KID:       0.3666\n✅ LPIPS:     0.6556\n✅ SSIM:      0.0434\n✅ PSNR:      5.9983\n✅ Diversity: 0.3025\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=1.0600, G_loss=7.5773]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/100 | D Loss: 0.9285 | G Loss: 2.8742\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 3 Metrics:\n✅ FID:       1959.3940\n✅ KID:       0.2715\n✅ LPIPS:     0.5782\n✅ SSIM:      0.0617\n✅ PSNR:      7.5356\n✅ Diversity: 0.3842\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=2.0797, G_loss=9.1750]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/100 | D Loss: 0.8671 | G Loss: 3.4445\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 4 Metrics:\n✅ FID:       1542.9953\n✅ KID:       0.2022\n✅ LPIPS:     0.6574\n✅ SSIM:      0.0622\n✅ PSNR:      8.3943\n✅ Diversity: 0.4104\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5785, G_loss=5.4929]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/100 | D Loss: 0.8224 | G Loss: 3.6558\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 5 Metrics:\n✅ FID:       1511.9704\n✅ KID:       0.2230\n✅ LPIPS:     0.4819\n✅ SSIM:      0.0892\n✅ PSNR:      9.0847\n✅ Diversity: 0.3750\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.6675, G_loss=5.3424]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/100 | D Loss: 0.7600 | G Loss: 4.0375\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 6 Metrics:\n✅ FID:       1576.9411\n✅ KID:       0.2052\n✅ LPIPS:     0.5610\n✅ SSIM:      0.1252\n✅ PSNR:      10.0474\n✅ Diversity: 0.4259\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5474, G_loss=5.8346]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/100 | D Loss: 0.7161 | G Loss: 4.0528\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 7 Metrics:\n✅ FID:       1533.6964\n✅ KID:       0.4432\n✅ LPIPS:     0.5751\n✅ SSIM:      0.0970\n✅ PSNR:      9.2674\n✅ Diversity: 0.4687\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4828, G_loss=4.0374]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/100 | D Loss: 0.6834 | G Loss: 4.4888\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 8 Metrics:\n✅ FID:       1926.7966\n✅ KID:       0.3528\n✅ LPIPS:     0.6427\n✅ SSIM:      0.1209\n✅ PSNR:      8.7406\n✅ Diversity: 0.4289\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.8215, G_loss=5.6645] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/100 | D Loss: 0.6902 | G Loss: 4.2957\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 9 Metrics:\n✅ FID:       1701.3787\n✅ KID:       0.2445\n✅ LPIPS:     0.4956\n✅ SSIM:      0.2143\n✅ PSNR:      11.4250\n✅ Diversity: 0.4088\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4575, G_loss=5.6138]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/100 | D Loss: 0.6333 | G Loss: 4.6299\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 10 Metrics:\n✅ FID:       1516.2939\n✅ KID:       0.2117\n✅ LPIPS:     0.4550\n✅ SSIM:      0.2784\n✅ PSNR:      10.7251\n✅ Diversity: 0.3648\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5469, G_loss=2.5086] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/100 | D Loss: 0.6228 | G Loss: 4.5617\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 11 Metrics:\n✅ FID:       1465.6065\n✅ KID:       0.2435\n✅ LPIPS:     0.5937\n✅ SSIM:      0.1570\n✅ PSNR:      8.9524\n✅ Diversity: 0.4391\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=1.5943, G_loss=6.3668] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/100 | D Loss: 0.6462 | G Loss: 4.4150\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 12 Metrics:\n✅ FID:       1519.7525\n✅ KID:       0.1482\n✅ LPIPS:     0.4033\n✅ SSIM:      0.2331\n✅ PSNR:      10.9477\n✅ Diversity: 0.3565\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=1.2113, G_loss=0.6657] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/100 | D Loss: 0.6628 | G Loss: 4.1838\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 13 Metrics:\n✅ FID:       1346.2574\n✅ KID:       0.1419\n✅ LPIPS:     0.4693\n✅ SSIM:      0.2562\n✅ PSNR:      10.2311\n✅ Diversity: 0.4681\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.7263, G_loss=4.3042]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/100 | D Loss: 0.7072 | G Loss: 4.2227\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 14 Metrics:\n✅ FID:       1443.4904\n✅ KID:       0.2829\n✅ LPIPS:     0.4990\n✅ SSIM:      0.2210\n✅ PSNR:      10.5554\n✅ Diversity: 0.4543\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.9298, G_loss=3.0084]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/100 | D Loss: 0.7875 | G Loss: 3.6312\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 15 Metrics:\n✅ FID:       1410.8825\n✅ KID:       0.2068\n✅ LPIPS:     0.4827\n✅ SSIM:      0.2098\n✅ PSNR:      10.6401\n✅ Diversity: 0.3614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.6257, G_loss=1.3900]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/100 | D Loss: 0.7894 | G Loss: 3.4350\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 16 Metrics:\n✅ FID:       1408.4612\n✅ KID:       0.1523\n✅ LPIPS:     0.4546\n✅ SSIM:      0.2521\n✅ PSNR:      10.6649\n✅ Diversity: 0.4499\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4728, G_loss=6.0511]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/100 | D Loss: 0.7711 | G Loss: 3.4358\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 17 Metrics:\n✅ FID:       1171.6934\n✅ KID:       0.1339\n✅ LPIPS:     0.4281\n✅ SSIM:      0.2348\n✅ PSNR:      10.7769\n✅ Diversity: 0.4294\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.6137, G_loss=5.2359]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/100 | D Loss: 0.7520 | G Loss: 3.5598\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 18 Metrics:\n✅ FID:       1265.1753\n✅ KID:       0.1558\n✅ LPIPS:     0.4113\n✅ SSIM:      0.2077\n✅ PSNR:      9.7074\n✅ Diversity: 0.3606\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5789, G_loss=2.6431]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/100 | D Loss: 0.7515 | G Loss: 3.3633\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 19 Metrics:\n✅ FID:       1278.8920\n✅ KID:       0.2870\n✅ LPIPS:     0.4026\n✅ SSIM:      0.2405\n✅ PSNR:      10.7389\n✅ Diversity: 0.3926\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.6284, G_loss=5.9749]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/100 | D Loss: 0.7869 | G Loss: 3.2590\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 20 Metrics:\n✅ FID:       1119.5379\n✅ KID:       0.1746\n✅ LPIPS:     0.4158\n✅ SSIM:      0.2865\n✅ PSNR:      10.3129\n✅ Diversity: 0.4208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5629, G_loss=2.6245]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/100 | D Loss: 0.7614 | G Loss: 3.1928\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 21 Metrics:\n✅ FID:       1389.7283\n✅ KID:       0.1616\n✅ LPIPS:     0.4149\n✅ SSIM:      0.2388\n✅ PSNR:      10.0911\n✅ Diversity: 0.3790\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5262, G_loss=2.3154]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/100 | D Loss: 0.7512 | G Loss: 3.2763\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 22 Metrics:\n✅ FID:       1416.8513\n✅ KID:       0.2155\n✅ LPIPS:     0.4301\n✅ SSIM:      0.2545\n✅ PSNR:      10.1691\n✅ Diversity: 0.3990\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5194, G_loss=2.3020]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/100 | D Loss: 0.7183 | G Loss: 3.2667\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 23 Metrics:\n✅ FID:       1227.3864\n✅ KID:       0.1363\n✅ LPIPS:     0.3644\n✅ SSIM:      0.3075\n✅ PSNR:      11.3265\n✅ Diversity: 0.3654\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4943, G_loss=3.6746]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/100 | D Loss: 0.7196 | G Loss: 3.4921\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 24 Metrics:\n✅ FID:       1179.5949\n✅ KID:       0.1157\n✅ LPIPS:     0.3856\n✅ SSIM:      0.2896\n✅ PSNR:      10.4019\n✅ Diversity: 0.3495\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5001, G_loss=6.9839]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/100 | D Loss: 0.7102 | G Loss: 3.4593\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 25 Metrics:\n✅ FID:       1350.9324\n✅ KID:       0.1463\n✅ LPIPS:     0.3817\n✅ SSIM:      0.2548\n✅ PSNR:      10.8454\n✅ Diversity: 0.3065\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4306, G_loss=4.5688]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26/100 | D Loss: 0.6334 | G Loss: 3.6549\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 26 Metrics:\n✅ FID:       1239.9656\n✅ KID:       0.1285\n✅ LPIPS:     0.4227\n✅ SSIM:      0.2183\n✅ PSNR:      9.8593\n✅ Diversity: 0.3816\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4495, G_loss=2.9604]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27/100 | D Loss: 0.6417 | G Loss: 3.7051\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 27 Metrics:\n✅ FID:       1432.9067\n✅ KID:       0.1213\n✅ LPIPS:     0.3779\n✅ SSIM:      0.2712\n✅ PSNR:      11.2069\n✅ Diversity: 0.3202\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4629, G_loss=3.6397]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/100 | D Loss: 0.6577 | G Loss: 3.6574\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 28 Metrics:\n✅ FID:       1173.5246\n✅ KID:       0.1625\n✅ LPIPS:     0.3955\n✅ SSIM:      0.2768\n✅ PSNR:      10.3098\n✅ Diversity: 0.3872\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.6026, G_loss=4.0508]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/100 | D Loss: 0.6311 | G Loss: 3.7689\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 29 Metrics:\n✅ FID:       1324.8043\n✅ KID:       0.2376\n✅ LPIPS:     0.3780\n✅ SSIM:      0.3020\n✅ PSNR:      11.0041\n✅ Diversity: 0.3464\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.6193, G_loss=4.4537]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/100 | D Loss: 0.5774 | G Loss: 3.9116\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 30 Metrics:\n✅ FID:       1309.7303\n✅ KID:       0.1568\n✅ LPIPS:     0.3707\n✅ SSIM:      0.2786\n✅ PSNR:      11.1351\n✅ Diversity: 0.3150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4465, G_loss=2.6735]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31/100 | D Loss: 0.6193 | G Loss: 3.9719\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 31 Metrics:\n✅ FID:       1294.5481\n✅ KID:       0.1473\n✅ LPIPS:     0.3833\n✅ SSIM:      0.2548\n✅ PSNR:      11.2653\n✅ Diversity: 0.4098\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5106, G_loss=2.3641]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32/100 | D Loss: 0.6099 | G Loss: 3.8155\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 32 Metrics:\n✅ FID:       1183.2083\n✅ KID:       0.1041\n✅ LPIPS:     0.3482\n✅ SSIM:      0.2649\n✅ PSNR:      10.1741\n✅ Diversity: 0.3934\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5544, G_loss=4.7089]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33/100 | D Loss: 0.5720 | G Loss: 4.0037\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 33 Metrics:\n✅ FID:       1185.2160\n✅ KID:       0.0905\n✅ LPIPS:     0.3621\n✅ SSIM:      0.2769\n✅ PSNR:      10.9326\n✅ Diversity: 0.3948\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4769, G_loss=2.8466]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34/100 | D Loss: 0.5737 | G Loss: 4.0169\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 34 Metrics:\n✅ FID:       1194.7711\n✅ KID:       0.1301\n✅ LPIPS:     0.3772\n✅ SSIM:      0.2780\n✅ PSNR:      11.5584\n✅ Diversity: 0.3691\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.7885, G_loss=2.7389]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35/100 | D Loss: 0.5631 | G Loss: 4.1629\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 35 Metrics:\n✅ FID:       1100.7096\n✅ KID:       0.1305\n✅ LPIPS:     0.3970\n✅ SSIM:      0.2228\n✅ PSNR:      10.4811\n✅ Diversity: 0.3144\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3973, G_loss=3.5642]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36/100 | D Loss: 0.5590 | G Loss: 4.1216\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 36 Metrics:\n✅ FID:       1316.7771\n✅ KID:       0.1156\n✅ LPIPS:     0.4324\n✅ SSIM:      0.2448\n✅ PSNR:      10.3895\n✅ Diversity: 0.3524\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4197, G_loss=6.0509]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37/100 | D Loss: 0.5366 | G Loss: 4.3558\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 37 Metrics:\n✅ FID:       1351.2371\n✅ KID:       0.1445\n✅ LPIPS:     0.3984\n✅ SSIM:      0.2529\n✅ PSNR:      10.1832\n✅ Diversity: 0.3519\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=1.0138, G_loss=1.3844]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38/100 | D Loss: 0.5783 | G Loss: 4.1310\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 38 Metrics:\n✅ FID:       1354.5265\n✅ KID:       0.1596\n✅ LPIPS:     0.4378\n✅ SSIM:      0.2457\n✅ PSNR:      10.3573\n✅ Diversity: 0.3299\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.8170, G_loss=1.2905]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39/100 | D Loss: 0.5683 | G Loss: 4.1364\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 39 Metrics:\n✅ FID:       1378.5998\n✅ KID:       0.1588\n✅ LPIPS:     0.4891\n✅ SSIM:      0.2167\n✅ PSNR:      10.6837\n✅ Diversity: 0.3594\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3762, G_loss=7.6364]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40/100 | D Loss: 0.5410 | G Loss: 4.3579\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 40 Metrics:\n✅ FID:       1157.0242\n✅ KID:       0.0954\n✅ LPIPS:     0.3623\n✅ SSIM:      0.3376\n✅ PSNR:      11.8793\n✅ Diversity: 0.4055\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4220, G_loss=3.5387]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41/100 | D Loss: 0.5097 | G Loss: 4.3601\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 41 Metrics:\n✅ FID:       1369.1501\n✅ KID:       0.0827\n✅ LPIPS:     0.4521\n✅ SSIM:      0.2357\n✅ PSNR:      10.2331\n✅ Diversity: 0.3576\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4218, G_loss=3.4887]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42/100 | D Loss: 0.5026 | G Loss: 4.5871\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 42 Metrics:\n✅ FID:       1185.5577\n✅ KID:       0.1609\n✅ LPIPS:     0.3732\n✅ SSIM:      0.3017\n✅ PSNR:      11.4394\n✅ Diversity: 0.3362\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4508, G_loss=6.0203]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43/100 | D Loss: 0.5123 | G Loss: 4.5057\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 43 Metrics:\n✅ FID:       1344.7990\n✅ KID:       0.1788\n✅ LPIPS:     0.3585\n✅ SSIM:      0.2916\n✅ PSNR:      10.6461\n✅ Diversity: 0.3584\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4056, G_loss=5.2867]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44/100 | D Loss: 0.5011 | G Loss: 4.4652\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 44 Metrics:\n✅ FID:       1271.6000\n✅ KID:       0.1265\n✅ LPIPS:     0.3662\n✅ SSIM:      0.2738\n✅ PSNR:      10.5936\n✅ Diversity: 0.3696\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5813, G_loss=2.2226]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45/100 | D Loss: 0.5087 | G Loss: 4.5323\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 45 Metrics:\n✅ FID:       1144.1743\n✅ KID:       0.1101\n✅ LPIPS:     0.3505\n✅ SSIM:      0.2826\n✅ PSNR:      9.6801\n✅ Diversity: 0.3725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5585, G_loss=1.9953]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46/100 | D Loss: 0.5409 | G Loss: 4.4910\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 46 Metrics:\n✅ FID:       1135.4970\n✅ KID:       0.1294\n✅ LPIPS:     0.4134\n✅ SSIM:      0.2684\n✅ PSNR:      10.1386\n✅ Diversity: 0.4247\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=1.0251, G_loss=1.4528]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47/100 | D Loss: 0.5440 | G Loss: 4.3832\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 47 Metrics:\n✅ FID:       1232.5908\n✅ KID:       0.1109\n✅ LPIPS:     0.3758\n✅ SSIM:      0.2759\n✅ PSNR:      10.5846\n✅ Diversity: 0.3981\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4910, G_loss=4.0596]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48/100 | D Loss: 0.4903 | G Loss: 4.6479\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 48 Metrics:\n✅ FID:       1191.8930\n✅ KID:       0.1117\n✅ LPIPS:     0.3872\n✅ SSIM:      0.2831\n✅ PSNR:      10.1810\n✅ Diversity: 0.3405\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5861, G_loss=4.3833]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49/100 | D Loss: 0.4729 | G Loss: 4.6955\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 49 Metrics:\n✅ FID:       1337.4865\n✅ KID:       0.0736\n✅ LPIPS:     0.3418\n✅ SSIM:      0.2641\n✅ PSNR:      9.5123\n✅ Diversity: 0.3614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5266, G_loss=5.2126]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50/100 | D Loss: 0.4804 | G Loss: 4.6761\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 50 Metrics:\n✅ FID:       1040.3468\n✅ KID:       0.0632\n✅ LPIPS:     0.4205\n✅ SSIM:      0.2452\n✅ PSNR:      10.2283\n✅ Diversity: 0.4176\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.3991, G_loss=6.0296]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51/100 | D Loss: 0.4893 | G Loss: 4.6466\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 51 Metrics:\n✅ FID:       1291.7513\n✅ KID:       0.0977\n✅ LPIPS:     0.4147\n✅ SSIM:      0.2414\n✅ PSNR:      9.9255\n✅ Diversity: 0.3312\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5327, G_loss=3.8753]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52/100 | D Loss: 0.5168 | G Loss: 4.6017\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 52 Metrics:\n✅ FID:       1323.2666\n✅ KID:       0.1248\n✅ LPIPS:     0.4232\n✅ SSIM:      0.2363\n✅ PSNR:      10.5697\n✅ Diversity: 0.3615\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5229, G_loss=2.8745]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53/100 | D Loss: 0.5346 | G Loss: 4.5914\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 53 Metrics:\n✅ FID:       1223.4912\n✅ KID:       0.1844\n✅ LPIPS:     0.3931\n✅ SSIM:      0.2543\n✅ PSNR:      10.3000\n✅ Diversity: 0.3552\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.7224, G_loss=0.8254] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 54/100 | D Loss: 0.5141 | G Loss: 4.6172\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 54 Metrics:\n✅ FID:       1169.0842\n✅ KID:       0.0983\n✅ LPIPS:     0.4075\n✅ SSIM:      0.2824\n✅ PSNR:      10.7356\n✅ Diversity: 0.3643\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4058, G_loss=5.5602]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55/100 | D Loss: 0.4811 | G Loss: 4.6484\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 55 Metrics:\n✅ FID:       1150.5875\n✅ KID:       0.0841\n✅ LPIPS:     0.3442\n✅ SSIM:      0.2392\n✅ PSNR:      11.0735\n✅ Diversity: 0.3808\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4856, G_loss=3.1167] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 56/100 | D Loss: 0.4618 | G Loss: 4.9541\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 56 Metrics:\n✅ FID:       1251.6843\n✅ KID:       0.0741\n✅ LPIPS:     0.3813\n✅ SSIM:      0.2877\n✅ PSNR:      10.8744\n✅ Diversity: 0.3537\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5493, G_loss=2.1909]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57/100 | D Loss: 0.4853 | G Loss: 4.9541\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 57 Metrics:\n✅ FID:       957.0740\n✅ KID:       0.0702\n✅ LPIPS:     0.3439\n✅ SSIM:      0.2822\n✅ PSNR:      11.0954\n✅ Diversity: 0.3488\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.9987, G_loss=0.5402]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58/100 | D Loss: 0.4570 | G Loss: 5.0400\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 58 Metrics:\n✅ FID:       1103.2994\n✅ KID:       0.0451\n✅ LPIPS:     0.4176\n✅ SSIM:      0.2440\n✅ PSNR:      10.1161\n✅ Diversity: 0.3519\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4478, G_loss=5.8383]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59/100 | D Loss: 0.5000 | G Loss: 4.8515\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 59 Metrics:\n✅ FID:       1090.2889\n✅ KID:       0.0921\n✅ LPIPS:     0.3835\n✅ SSIM:      0.2412\n✅ PSNR:      10.3167\n✅ Diversity: 0.3898\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4939, G_loss=4.7093] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 60/100 | D Loss: 0.5025 | G Loss: 4.9362\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 60 Metrics:\n✅ FID:       1287.7162\n✅ KID:       0.1558\n✅ LPIPS:     0.4270\n✅ SSIM:      0.2294\n✅ PSNR:      10.1315\n✅ Diversity: 0.3769\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4419, G_loss=3.3719]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61/100 | D Loss: 0.5001 | G Loss: 4.5076\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 61 Metrics:\n✅ FID:       1257.9328\n✅ KID:       0.1411\n✅ LPIPS:     0.4325\n✅ SSIM:      0.2599\n✅ PSNR:      10.9215\n✅ Diversity: 0.3444\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4520, G_loss=7.6352] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 62/100 | D Loss: 0.4688 | G Loss: 4.9587\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 62 Metrics:\n✅ FID:       1232.1542\n✅ KID:       0.1063\n✅ LPIPS:     0.3825\n✅ SSIM:      0.2463\n✅ PSNR:      10.2186\n✅ Diversity: 0.3520\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4405, G_loss=7.4588]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63/100 | D Loss: 0.4545 | G Loss: 5.0963\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 63 Metrics:\n✅ FID:       1168.5007\n✅ KID:       0.1074\n✅ LPIPS:     0.3765\n✅ SSIM:      0.2820\n✅ PSNR:      10.7264\n✅ Diversity: 0.3114\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4058, G_loss=5.1008]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64/100 | D Loss: 0.4462 | G Loss: 5.0004\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 64 Metrics:\n✅ FID:       1056.5407\n✅ KID:       0.0679\n✅ LPIPS:     0.3945\n✅ SSIM:      0.2353\n✅ PSNR:      10.7689\n✅ Diversity: 0.3113\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5875, G_loss=1.4832] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 65/100 | D Loss: 0.4485 | G Loss: 5.2092\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 65 Metrics:\n✅ FID:       1128.2887\n✅ KID:       0.0737\n✅ LPIPS:     0.3479\n✅ SSIM:      0.2885\n✅ PSNR:      11.0793\n✅ Diversity: 0.3701\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.7319, G_loss=0.7867] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 66/100 | D Loss: 0.4900 | G Loss: 5.4497\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 66 Metrics:\n✅ FID:       1133.8910\n✅ KID:       0.0650\n✅ LPIPS:     0.3500\n✅ SSIM:      0.3075\n✅ PSNR:      10.5683\n✅ Diversity: 0.2994\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4893, G_loss=3.8261]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67/100 | D Loss: 0.4625 | G Loss: 5.1587\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 67 Metrics:\n✅ FID:       1069.2237\n✅ KID:       0.0799\n✅ LPIPS:     0.4147\n✅ SSIM:      0.3028\n✅ PSNR:      11.4689\n✅ Diversity: 0.4021\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4185, G_loss=4.0618] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 68/100 | D Loss: 0.4569 | G Loss: 5.1231\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 68 Metrics:\n✅ FID:       1160.3653\n✅ KID:       0.0578\n✅ LPIPS:     0.4321\n✅ SSIM:      0.2947\n✅ PSNR:      10.6190\n✅ Diversity: 0.4256\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4853, G_loss=4.0225] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 69/100 | D Loss: 0.4439 | G Loss: 5.2530\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 69 Metrics:\n✅ FID:       1144.5154\n✅ KID:       0.0518\n✅ LPIPS:     0.4952\n✅ SSIM:      0.2438\n✅ PSNR:      9.8819\n✅ Diversity: 0.4639\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4594, G_loss=4.1658]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70/100 | D Loss: 0.4605 | G Loss: 5.2530\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 70 Metrics:\n✅ FID:       1232.0186\n✅ KID:       0.0995\n✅ LPIPS:     0.4348\n✅ SSIM:      0.2839\n✅ PSNR:      10.5519\n✅ Diversity: 0.3848\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4783, G_loss=6.9288] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 71/100 | D Loss: 0.4504 | G Loss: 5.5214\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 71 Metrics:\n✅ FID:       1107.8008\n✅ KID:       0.1308\n✅ LPIPS:     0.4079\n✅ SSIM:      0.2495\n✅ PSNR:      9.5731\n✅ Diversity: 0.3727\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3796, G_loss=7.7316]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72/100 | D Loss: 0.4308 | G Loss: 5.2083\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 72 Metrics:\n✅ FID:       1181.2611\n✅ KID:       0.0664\n✅ LPIPS:     0.3882\n✅ SSIM:      0.2389\n✅ PSNR:      9.5609\n✅ Diversity: 0.3499\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.7420, G_loss=0.5322] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 73/100 | D Loss: 0.4512 | G Loss: 5.3556\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 73 Metrics:\n✅ FID:       1154.7889\n✅ KID:       0.1313\n✅ LPIPS:     0.3455\n✅ SSIM:      0.3072\n✅ PSNR:      9.9459\n✅ Diversity: 0.3281\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4244, G_loss=6.8852] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 74/100 | D Loss: 0.4474 | G Loss: 5.4764\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 74 Metrics:\n✅ FID:       1225.7561\n✅ KID:       0.0284\n✅ LPIPS:     0.4048\n✅ SSIM:      0.3086\n✅ PSNR:      10.8543\n✅ Diversity: 0.3966\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4727, G_loss=3.0533] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 75/100 | D Loss: 0.4264 | G Loss: 5.4122\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 75 Metrics:\n✅ FID:       1228.9935\n✅ KID:       0.0473\n✅ LPIPS:     0.3913\n✅ SSIM:      0.3346\n✅ PSNR:      11.3634\n✅ Diversity: 0.3689\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4188, G_loss=4.1232]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76/100 | D Loss: 0.4262 | G Loss: 5.5654\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 76 Metrics:\n✅ FID:       1160.2662\n✅ KID:       0.0524\n✅ LPIPS:     0.3496\n✅ SSIM:      0.2682\n✅ PSNR:      11.0071\n✅ Diversity: 0.3218\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3744, G_loss=6.2972]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77/100 | D Loss: 0.4165 | G Loss: 5.6381\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 77 Metrics:\n✅ FID:       1082.9713\n✅ KID:       0.0824\n✅ LPIPS:     0.4321\n✅ SSIM:      0.2700\n✅ PSNR:      10.1121\n✅ Diversity: 0.4053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=1.0991, G_loss=0.4116] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 78/100 | D Loss: 0.4346 | G Loss: 5.7084\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 78 Metrics:\n✅ FID:       1074.7749\n✅ KID:       0.0765\n✅ LPIPS:     0.4246\n✅ SSIM:      0.2005\n✅ PSNR:      9.7520\n✅ Diversity: 0.3681\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4175, G_loss=8.5757] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 79/100 | D Loss: 0.4909 | G Loss: 5.4842\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 79 Metrics:\n✅ FID:       898.5637\n✅ KID:       0.0475\n✅ LPIPS:     0.3949\n✅ SSIM:      0.3162\n✅ PSNR:      10.2854\n✅ Diversity: 0.4918\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.5099, G_loss=2.6957] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 80/100 | D Loss: 0.4572 | G Loss: 5.7250\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 80 Metrics:\n✅ FID:       1081.3836\n✅ KID:       0.0618\n✅ LPIPS:     0.3380\n✅ SSIM:      0.2963\n✅ PSNR:      11.5666\n✅ Diversity: 0.4154\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3848, G_loss=5.5935] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 81/100 | D Loss: 0.4303 | G Loss: 5.8943\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 81 Metrics:\n✅ FID:       1106.0488\n✅ KID:       0.0950\n✅ LPIPS:     0.3961\n✅ SSIM:      0.3023\n✅ PSNR:      11.1593\n✅ Diversity: 0.3679\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4081, G_loss=5.5061] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 82/100 | D Loss: 0.4283 | G Loss: 5.8233\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 82 Metrics:\n✅ FID:       1295.8134\n✅ KID:       0.1309\n✅ LPIPS:     0.4504\n✅ SSIM:      0.2413\n✅ PSNR:      9.9590\n✅ Diversity: 0.4470\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4384, G_loss=4.2403] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 83/100 | D Loss: 0.4319 | G Loss: 5.5904\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 83 Metrics:\n✅ FID:       1079.1234\n✅ KID:       0.1371\n✅ LPIPS:     0.4156\n✅ SSIM:      0.2812\n✅ PSNR:      10.9484\n✅ Diversity: 0.3783\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3903, G_loss=6.2096]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84/100 | D Loss: 0.4358 | G Loss: 5.6702\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 84 Metrics:\n✅ FID:       1061.8539\n✅ KID:       0.0696\n✅ LPIPS:     0.4068\n✅ SSIM:      0.2899\n✅ PSNR:      10.4206\n✅ Diversity: 0.4303\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4112, G_loss=4.2462] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 85/100 | D Loss: 0.4441 | G Loss: 5.6206\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 85 Metrics:\n✅ FID:       1142.8318\n✅ KID:       0.1286\n✅ LPIPS:     0.4365\n✅ SSIM:      0.2904\n✅ PSNR:      11.2258\n✅ Diversity: 0.4203\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4672, G_loss=3.9870] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 86/100 | D Loss: 0.4275 | G Loss: 6.0442\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 86 Metrics:\n✅ FID:       1199.2449\n✅ KID:       0.1203\n✅ LPIPS:     0.4392\n✅ SSIM:      0.2874\n✅ PSNR:      10.3113\n✅ Diversity: 0.3975\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4686, G_loss=6.4480]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87/100 | D Loss: 0.4220 | G Loss: 6.0627\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 87 Metrics:\n✅ FID:       1392.2339\n✅ KID:       0.1444\n✅ LPIPS:     0.3859\n✅ SSIM:      0.2520\n✅ PSNR:      10.3219\n✅ Diversity: 0.4016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4341, G_loss=5.0597] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 88/100 | D Loss: 0.4400 | G Loss: 5.9912\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 88 Metrics:\n✅ FID:       1090.6237\n✅ KID:       0.0505\n✅ LPIPS:     0.4834\n✅ SSIM:      0.2473\n✅ PSNR:      9.7213\n✅ Diversity: 0.4216\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.3958, G_loss=7.8572] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 89/100 | D Loss: 0.4609 | G Loss: 5.5473\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 89 Metrics:\n✅ FID:       1226.1231\n✅ KID:       0.0726\n✅ LPIPS:     0.3711\n✅ SSIM:      0.2712\n✅ PSNR:      11.2382\n✅ Diversity: 0.3313\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4899, G_loss=7.5546] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 90/100 | D Loss: 0.4314 | G Loss: 5.6274\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 90 Metrics:\n✅ FID:       949.8113\n✅ KID:       0.0080\n✅ LPIPS:     0.3482\n✅ SSIM:      0.2757\n✅ PSNR:      10.9744\n✅ Diversity: 0.3762\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.6457, G_loss=0.6676]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91/100 | D Loss: 0.4137 | G Loss: 5.6499\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 91 Metrics:\n✅ FID:       1082.5909\n✅ KID:       0.0366\n✅ LPIPS:     0.3843\n✅ SSIM:      0.2506\n✅ PSNR:      10.2176\n✅ Diversity: 0.3676\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.5648, G_loss=5.8074] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 92/100 | D Loss: 0.4783 | G Loss: 5.2682\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 92 Metrics:\n✅ FID:       1402.1606\n✅ KID:       0.1568\n✅ LPIPS:     0.4810\n✅ SSIM:      0.2205\n✅ PSNR:      10.2454\n✅ Diversity: 0.5251\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4341, G_loss=7.0792] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 93/100 | D Loss: 0.4672 | G Loss: 5.2240\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 93 Metrics:\n✅ FID:       969.7166\n✅ KID:       0.0977\n✅ LPIPS:     0.4033\n✅ SSIM:      0.2433\n✅ PSNR:      10.7078\n✅ Diversity: 0.4119\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4179, G_loss=6.8955] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 94/100 | D Loss: 0.4349 | G Loss: 5.5936\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 94 Metrics:\n✅ FID:       1146.5040\n✅ KID:       0.1185\n✅ LPIPS:     0.4158\n✅ SSIM:      0.2568\n✅ PSNR:      10.3979\n✅ Diversity: 0.4784\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4716, G_loss=3.1237] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 95/100 | D Loss: 0.4232 | G Loss: 5.6701\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 95 Metrics:\n✅ FID:       1131.1961\n✅ KID:       0.0785\n✅ LPIPS:     0.4175\n✅ SSIM:      0.2842\n✅ PSNR:      11.1255\n✅ Diversity: 0.3589\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.4270, G_loss=4.5834]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 96/100 | D Loss: 0.4132 | G Loss: 5.8199\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 96 Metrics:\n✅ FID:       1274.6372\n✅ KID:       0.0733\n✅ LPIPS:     0.3662\n✅ SSIM:      0.2776\n✅ PSNR:      10.0959\n✅ Diversity: 0.3649\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=1.5168, G_loss=0.7541] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 97/100 | D Loss: 0.4249 | G Loss: 5.9027\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 97 Metrics:\n✅ FID:       1297.5683\n✅ KID:       0.0620\n✅ LPIPS:     0.3928\n✅ SSIM:      0.3160\n✅ PSNR:      12.0420\n✅ Diversity: 0.3853\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.4308, G_loss=7.1292] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 98/100 | D Loss: 0.4703 | G Loss: 5.5979\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 98 Metrics:\n✅ FID:       1161.1420\n✅ KID:       0.0908\n✅ LPIPS:     0.3913\n✅ SSIM:      0.2503\n✅ PSNR:      10.6613\n✅ Diversity: 0.3412\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99/100: 100%|██████████| 224/224 [01:29<00:00,  2.51it/s, D_loss=0.3934, G_loss=3.9716] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 99/100 | D Loss: 0.4240 | G Loss: 5.7051\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 99 Metrics:\n✅ FID:       1064.1102\n✅ KID:       0.0128\n✅ LPIPS:     0.4758\n✅ SSIM:      0.2281\n✅ PSNR:      10.2272\n✅ Diversity: 0.5160\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100/100: 100%|██████████| 224/224 [01:29<00:00,  2.50it/s, D_loss=0.3789, G_loss=5.9963] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 100/100 | D Loss: 0.4248 | G Loss: 5.8377\nSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\nLoading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n\n📊 Epoch 100 Metrics:\n✅ FID:       956.6345\n✅ KID:       0.0544\n✅ LPIPS:     0.3491\n✅ SSIM:      0.2785\n✅ PSNR:      10.6165\n✅ Diversity: 0.3577\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"plot_training_metrics(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:23:01.790194Z","iopub.execute_input":"2025-05-05T20:23:01.790494Z","iopub.status.idle":"2025-05-05T20:23:02.505234Z","shell.execute_reply.started":"2025-05-05T20:23:01.790467Z","shell.execute_reply":"2025-05-05T20:23:02.504563Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\"\"\"import shutil\nimport os\n\nworking_dir = \"/kaggle/working\"\n\n# Loop through everything in the directory\nfor filename in os.listdir(working_dir):\n    file_path = os.path.join(working_dir, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)  # remove file or symbolic link\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)  # remove directory\n    except Exception as e:\n        print(f\"Failed to delete {file_path}. Reason: {e}\")\n\nprint(\"✅ All files and folders in /kaggle/working deleted.\")\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:23:02.506099Z","iopub.execute_input":"2025-05-05T20:23:02.506363Z","iopub.status.idle":"2025-05-05T20:23:02.512501Z","shell.execute_reply.started":"2025-05-05T20:23:02.506340Z","shell.execute_reply":"2025-05-05T20:23:02.511690Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'import shutil\\nimport os\\n\\nworking_dir = \"/kaggle/working\"\\n\\n# Loop through everything in the directory\\nfor filename in os.listdir(working_dir):\\n    file_path = os.path.join(working_dir, filename)\\n    try:\\n        if os.path.isfile(file_path) or os.path.islink(file_path):\\n            os.unlink(file_path)  # remove file or symbolic link\\n        elif os.path.isdir(file_path):\\n            shutil.rmtree(file_path)  # remove directory\\n    except Exception as e:\\n        print(f\"Failed to delete {file_path}. Reason: {e}\")\\n\\nprint(\"✅ All files and folders in /kaggle/working deleted.\")'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
